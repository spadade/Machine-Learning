---
title: "Machine Learning - Heirarchical Clustering"
author: "SrushtiP"
date: "11/10/2019"
output: html_document
---

Problem Statement:

The dataset Cereals.csv includes nutritional information, store display, and consumer ratings for 77 breakfast cereals.

Data Preprocessing. Remove all cereals with missing values

Libraries required throughout the assignment.

```{r}
library(ISLR)
library(caret)
library(cluster)
library(factoextra)
```
Reading  the data from the dataset and changing the Name column as the row name.

```{r}
Cereals <- read.csv("Cereals.csv")

rownames(Cereals) <- Cereals$name

Cereals <- Cereals[-1]
View(Cereals)
```
Normalisation of the data is usually performed on the quantitative data. Hence, here we are normalizing the quantitative data and keeping the catagorical variables as it is.

```{r}
Cereals_Quant <- scale(Cereals[c(-1,-2,-12)])
Cereals_Quant <- cbind(Cereals[c(1,2,12)],Cereals_Quant)
```
Omititng the NA values from the dataset.

```{r}
Cereals_Quant <- na.omit(Cereals_Quant)
summary(Cereals_Quant) 
```

1. ) Apply hierarchical clustering to the data using Euclidean distance to the normalized measurements. Use Agnes to compare the clustering from  single linkage, complete linkage, average linkage, and Ward. Choose the best method.

```{r}
SingleLinkage <- agnes(Cereals_Quant, method = "single")
SingleLinkage$ac

CompleteLinkage <- agnes(Cereals_Quant, method = "complete")
CompleteLinkage$ac

AverageLinkage <- agnes(Cereals_Quant, method = "average")
AverageLinkage$ac

WardLinkage <- agnes(Cereals_Quant, method = "ward")
WardLinkage$ac
```
As we look at the above linkages, we can see that the Ward Linkage is highest. Hence we can use the Ward linkage to form the clusters.

We are using the Euclideans Distance to compute the distance and assign elements to each cluster.

```{r}
Euclideans <- dist(Cereals_Quant, method = "euclidean")

fviz_dist(Euclideans)

WardLinkage <- hclust(Euclideans, method = "ward.D2")
```

2.)	How many clusters would you choose?

Now, we plot the dendrogram and try to find the optimal value for k.

Lets consider K = 2:

```{r}
par(mar=c(1,1,1,1))

plot(WardLinkage, cex = 0.6, hang = -1)
rect.hclust(WardLinkage, k = 2, border = 1:2)
```
Using k = 2, we can see that the content of each cluster is almost same which cannot distinguish any cluster. Thus k = 2 may not be a good choice.

Now, consider k = 4:

```{r}
plot(WardLinkage, cex = 0.6, hang = -1)
rect.hclust(WardLinkage, k = 4, border = 1:4)
```

Here considering k= 4, we are facing a problem with the highly uneven heights of the nutrients content so the cluster formed and not exactly optimal clusters. Thus, k = 4 is also not a good choice.

Now let us consider k = 6:

```{r}
plot(WardLinkage, cex = 0.6, hang = -1)
rect.hclust(WardLinkage, k = 6, border = 1:6)
```

As discussed above, the height of each branch in the dendrogram represents the nutrition level of the various cereals. As Clustering takes place considering all the paramenters, here which are the nutrient contents in cereals. Using k=6 the height show stability with all the clusters i.e. between 5 to 10.
Also, the Cereal distribution is fair. we can distinguish the type of cereals for all the cluters.
Thus k = 6 canbe considers a good choice as number of clusters.

```{r}
group <- cutree(WardLinkage, k=6)
table(group)
```

The table represents the cereals in each clusters.

3.)	Comment on the structure of the clusters and on their stability. Hint: To check stability,  partition the data and see how well clusters formed based on one part apply to the other part. To do this:

  1.	Cluster partition A

```{r}
set.seed(15)
Training_Data_Index <- createDataPartition(Cereals$rating, p = 0.6, list = FALSE)

Training_Data <- Cereals[Training_Data_Index,]
Training_Data_N <- scale(Training_Data[c(-1,-2,-12)])
Training_Data_N <- cbind(Training_Data[c(1,2,12)],Training_Data_N)
Training_Data_N <- na.omit(Training_Data_N)

Validation_Data <- Cereals[-Training_Data_Index,]
Validation_Data_N <- scale(Validation_Data[c(-1,-2,-12)])
Validation_Data_N <- cbind(Validation_Data[c(1,2,12)],Validation_Data_N)
Validation_Data_N <- na.omit(Validation_Data_N)
```

  2.	Use the cluster centroids from A to assign each record in partition B (each record is assigned to the cluster with the closest centroid).

```{r}
set.seed(15)
Training_Data_Index <- createDataPartition(Cereals$rating, p = 0.6, list = FALSE)

Training_Data <- Cereals[Training_Data_Index,]
Training_Data_N <- scale(Training_Data[c(-1,-2,-12)])
Training_Data_N <- na.omit(Training_Data_N)

Validation_Data <- Cereals[-Training_Data_Index,]
Validation_Data_N <- scale(Validation_Data[c(-1,-2,-12)])
Validation_Data_N <- na.omit(Validation_Data_N)
```


```{r}
Train_WardLinkage <- agnes(Training_Data_N, method = "ward")
Valid_WardLinkage <- agnes(Validation_Data_N, method = "ward")

Euclideans_T <- dist(Training_Data_N, method = "euclidean")
Hclust_train <- hclust(Euclideans_T, method = "ward.D2")

Euclideans_V <- dist(Validation_Data_N, method = "euclidean")
Hclust_valid <- hclust(Euclideans_V, method = "ward.D2")
```

```{r}
k_means_T <- kmeans(Training_Data_N, centers = 6)

k_means_V <- kmeans(Validation_Data_N, centers = 6)
```

  3.	Assess how consistent the cluster assignments are compared to the assignments based on all the data.

```{r}

Training_Data_N <- cbind(Clusters = k_means_T$cluster, Training_Data_N)

Validation_Data_N <- cbind(Clusters = k_means_V$cluster, Validation_Data_N)
```

4.)	The elementary public schools would like to choose a set of cereals to include in their daily cafeterias. Every day a different cereal is offered, but all cereals should support a healthy diet. For this goal, you are requested to find a cluster of “healthy cereals.” Should the data be normalized? If not, how should they be used in the cluster analysis?

```{r}
Cereals_Quant <- cbind(Cereals_Quant, group)
plot(x=Cereals_Quant$group, y = Cereals_Quant$rating, xlab = "Clusters", ylab = "Cereals Rating", main = "Cluster vs Cereals Ratings")
```

Comparing the Dendograms there are 6 cluters formed. So based on the height of the nutrient contents in each of the cluster we can see that the nutrition is higher in cluster 6.

Also, by looking at the plot the ratings of the cereals vs the Cluster groups we see that the high number of data is in Cluster 6 with relatively high ratings.

```{r}
Breakfast <- Cereals_Quant[Cereals_Quant$group == '6',]

row.names(Breakfast)
```

Hence, the above cereals should be included in the breakfast meal plan for the school.

